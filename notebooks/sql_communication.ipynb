{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ffddcb",
   "metadata": {},
   "source": [
    "Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571be7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain import hub\n",
    "import llama_cpp\n",
    "from typing import TypedDict, Dict, List\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import StateGraph, START\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26e50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-06-25 14:13:45\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%d-%m-%y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4548a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to track flow\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "\n",
    "    query: str\n",
    "\n",
    "    result: str\n",
    "\n",
    "    report: str\n",
    "    answer: str\n",
    "\n",
    "    db: SQLDatabase\n",
    "    df_info: str\n",
    "    database_uri: str\n",
    "\n",
    "    route: int\n",
    "\n",
    "    chat_history: List[Dict[str, str]]\n",
    "\n",
    "#define structure for generated SQL prompt\n",
    "class QueryOutput(TypedDict):\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5743d14",
   "metadata": {},
   "source": [
    "Establishing database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f7e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db=SQLDatabase.from_uri(\"sqlite:///../sql_files/myDataBase.db\")\n",
    "sqlite_info=sqlite_db.get_table_info()\n",
    "\n",
    "def assign_db(state: State):\n",
    "    #return {\"db\": sqlite_db, \"db_info\": sqlite_info}\n",
    "    state[\"db\"]=sqlite_db\n",
    "    state[\"db_info\"]=sqlite_info\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c998dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.utilities.sql_database.SQLDatabase object at 0x000001E2985C8210>\n"
     ]
    }
   ],
   "source": [
    "print(sqlite_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0d6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE \"JANES_data\" (\n",
      "\tlatitude FLOAT, \n",
      "\tlongitude FLOAT, \n",
      "\tlocation_name TEXT, \n",
      "\tlocation_country TEXT, \n",
      "\tlocation TEXT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from JANES_data table:\n",
      "latitude\tlongitude\tlocation_name\tlocation_country\tlocation\n",
      "18.9393\t72.8445\tmumbai\tindia\t(18.9393, 72.8445)\n",
      "19.0056\t72.8163\tjawaharlal nehru port (nhava sheva)\tindia\t(19.0056, 72.8163)\n",
      "17.6856\t83.216\tvisakhapatnam\tindia\t(17.6856, 83.216)\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE \"OTAS_data\" (\n",
      "\tid BIGINT, \n",
      "\tname TEXT, \n",
      "\tlatitude FLOAT, \n",
      "\tlongitude FLOAT, \n",
      "\trange FLOAT, \n",
      "\tbearing FLOAT, \n",
      "\tcourse FLOAT, \n",
      "\tspeed FLOAT, \n",
      "\taltitude BIGINT, \n",
      "\tdepth BIGINT, \n",
      "\treported_by TEXT, \n",
      "\tcomment TEXT, \n",
      "\thostility TEXT, \n",
      "\tcategory TEXT, \n",
      "\tnationality TEXT, \n",
      "\tlocation_wrt_naval_borders TEXT, \n",
      "\tclosest_point_of_mil_interest TEXT, \n",
      "\ttime TEXT, \n",
      "\ttarget_location TEXT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from OTAS_data table:\n",
      "id\tname\tlatitude\tlongitude\trange\tbearing\tcourse\tspeed\taltitude\tdepth\treported_by\tcomment\thostility\tcategory\tnationality\tlocation_wrt_naval_borders\tclosest_point_of_mil_interest\ttime\ttarget_location\n",
      "2004\tghazi\t20.44\t69.23\t204.6\t300.7\t127.5\t5.4\t0\t0\tutkarsh\tseems to be approaching territorial waters\thostile\tsurface\tpakistani\tinside indian waters\tjawaharlal nehru port (nhava sheva)\t11:25:58\t(20.44, 69.23)\n",
      "2001\trafale\t19.5856\t67.2376\t138.0\t293.7\t149.7\t735.0\t120001\t0\tarnab\terratic movements\tfriendly\tair\tindian\tinside indian waters\tport qasim\t12:02:47\t(19.5856, 67.2376)\n",
      "2001\trafale\t19.5589\t70.16\t127.9\t290.3\t166.9\t385.0\t120001\t0\tarnab\terratic movements\tfriendly\tair\tindian\tinside indian waters\tjawaharlal nehru port (nhava sheva)\t12:03:47\t(19.5589, 70.16)\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "print(sqlite_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf4390",
   "metadata": {},
   "source": [
    "Initialising LLM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23682df",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5ef8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 77 key-value pairs and 255 tensors from ..//models//Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Dolphin 3.0 Llama 3.2 3B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv   4:                           general.basename str              = dolphin-3.0-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.2 3B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
      "llama_model_loader: - kv  11:                      general.dataset.count u32              = 13\n",
      "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Opc Sft Stage1\n",
      "llama_model_loader: - kv  13:             general.dataset.0.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  15:                     general.dataset.1.name str              = Opc Sft Stage2\n",
      "llama_model_loader: - kv  16:             general.dataset.1.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  17:                 general.dataset.1.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  18:                     general.dataset.2.name str              = Orca Agentinstruct 1M v1\n",
      "llama_model_loader: - kv  19:                  general.dataset.2.version str              = v1\n",
      "llama_model_loader: - kv  20:             general.dataset.2.organization str              = Microsoft\n",
      "llama_model_loader: - kv  21:                 general.dataset.2.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  22:                     general.dataset.3.name str              = Orca Math Word Problems 200k\n",
      "llama_model_loader: - kv  23:             general.dataset.3.organization str              = Microsoft\n",
      "llama_model_loader: - kv  24:                 general.dataset.3.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  25:                     general.dataset.4.name str              = Hermes Function Calling v1\n",
      "llama_model_loader: - kv  26:                  general.dataset.4.version str              = v1\n",
      "llama_model_loader: - kv  27:             general.dataset.4.organization str              = NousResearch\n",
      "llama_model_loader: - kv  28:                 general.dataset.4.repo_url str              = https://huggingface.co/NousResearch/h...\n",
      "llama_model_loader: - kv  29:                     general.dataset.5.name str              = NuminaMath CoT\n",
      "llama_model_loader: - kv  30:             general.dataset.5.organization str              = AI MO\n",
      "llama_model_loader: - kv  31:                 general.dataset.5.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  32:                     general.dataset.6.name str              = NuminaMath TIR\n",
      "llama_model_loader: - kv  33:             general.dataset.6.organization str              = AI MO\n",
      "llama_model_loader: - kv  34:                 general.dataset.6.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  35:                     general.dataset.7.name str              = Tulu 3 Sft Mixture\n",
      "llama_model_loader: - kv  36:             general.dataset.7.organization str              = Allenai\n",
      "llama_model_loader: - kv  37:                 general.dataset.7.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  38:                     general.dataset.8.name str              = Dolphin Coder\n",
      "llama_model_loader: - kv  39:             general.dataset.8.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  40:                 general.dataset.8.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  41:                     general.dataset.9.name str              = Smoltalk\n",
      "llama_model_loader: - kv  42:             general.dataset.9.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv  43:                 general.dataset.9.repo_url str              = https://huggingface.co/HuggingFaceTB/...\n",
      "llama_model_loader: - kv  44:                    general.dataset.10.name str              = Samantha Data\n",
      "llama_model_loader: - kv  45:            general.dataset.10.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  46:                general.dataset.10.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  47:                    general.dataset.11.name str              = CodeFeedback Filtered Instruction\n",
      "llama_model_loader: - kv  48:            general.dataset.11.organization str              = M A P\n",
      "llama_model_loader: - kv  49:                general.dataset.11.repo_url str              = https://huggingface.co/m-a-p/CodeFeed...\n",
      "llama_model_loader: - kv  50:                    general.dataset.12.name str              = Code Feedback\n",
      "llama_model_loader: - kv  51:            general.dataset.12.organization str              = M A P\n",
      "llama_model_loader: - kv  52:                general.dataset.12.repo_url str              = https://huggingface.co/m-a-p/Code-Fee...\n",
      "llama_model_loader: - kv  53:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  54:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  55:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  56:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  57:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  58:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  59:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  60:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  61:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  62:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  63:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  64:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  65:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  66:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  67:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  68:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  69:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  70:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  71:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  72:                tokenizer.ggml.eos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  73:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  74:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  75:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  76:                          general.file_type u32              = 17\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q5_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 2.16 GiB (5.76 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128257 '<|im_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 258\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Dolphin 3.0 Llama 3.2 3B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128258\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128256 '<|im_end|>'\n",
      "print_info: EOT token        = 128256 '<|im_end|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: EOG token        = 128256 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 282 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2207.11 MiB\n",
      "........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   896.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   424.01 MiB\n",
      "llama_context: graph nodes  = 958\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.dataset.10.repo_url': 'https://huggingface.co/cognitivecomputations/samantha-data', 'general.name': 'Dolphin 3.0 Llama 3.2 3B', 'general.dataset.9.repo_url': 'https://huggingface.co/HuggingFaceTB/smoltalk', 'general.dataset.8.name': 'Dolphin Coder', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Cognitivecomputations', 'general.basename': 'dolphin-3.0-Llama-3.2', 'general.dataset.0.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage1', 'general.dataset.12.name': 'Code Feedback', 'general.size_label': '3B', 'general.license': 'llama3.2', 'general.dataset.9.organization': 'HuggingFaceTB', 'general.base_model.count': '1', 'general.base_model.0.name': 'Llama 3.2 3B', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Llama-3.2-3B', 'general.dataset.2.name': 'Orca Agentinstruct 1M v1', 'general.dataset.11.organization': 'M A P', 'general.dataset.count': '13', 'llama.attention.key_length': '128', 'general.dataset.0.name': 'Opc Sft Stage1', 'llama.rope.freq_base': '500000.000000', 'general.dataset.5.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-CoT', 'general.dataset.4.name': 'Hermes Function Calling v1', 'general.dataset.0.organization': 'OpenCoder LLM', 'llama.vocab_size': '128258', 'general.dataset.1.name': 'Opc Sft Stage2', 'general.dataset.1.organization': 'OpenCoder LLM', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.dataset.1.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage2', 'general.dataset.2.version': 'v1', 'general.dataset.2.organization': 'Microsoft', 'general.dataset.12.organization': 'M A P', 'general.dataset.2.repo_url': 'https://huggingface.co/microsoft/orca-agentinstruct-1M-v1', 'general.dataset.3.name': 'Orca Math Word Problems 200k', 'general.dataset.3.organization': 'Microsoft', 'general.dataset.3.repo_url': 'https://huggingface.co/microsoft/orca-math-word-problems-200k', 'general.dataset.10.organization': 'Cognitivecomputations', 'general.dataset.4.version': 'v1', 'general.dataset.11.name': 'CodeFeedback Filtered Instruction', 'general.dataset.4.organization': 'NousResearch', 'general.dataset.5.name': 'NuminaMath CoT', 'general.dataset.4.repo_url': 'https://huggingface.co/NousResearch/hermes-function-calling-v1', 'general.dataset.5.organization': 'AI MO', 'general.dataset.6.name': 'NuminaMath TIR', 'general.dataset.6.organization': 'AI MO', 'general.dataset.6.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-TIR', 'general.dataset.7.name': 'Tulu 3 Sft Mixture', 'llama.feed_forward_length': '8192', 'general.dataset.7.organization': 'Allenai', 'general.dataset.7.repo_url': 'https://huggingface.co/allenai/tulu-3-sft-mixture', 'general.dataset.8.organization': 'Cognitivecomputations', 'general.dataset.8.repo_url': 'https://huggingface.co/cognitivecomputations/dolphin-coder', 'general.dataset.9.name': 'Smoltalk', 'general.dataset.10.name': 'Samantha Data', 'llama.block_count': '28', 'general.dataset.11.repo_url': 'https://huggingface.co/m-a-p/CodeFeedback-Filtered-Instruction', 'llama.attention.head_count_kv': '8', 'tokenizer.ggml.eos_token_id': '128256', 'general.dataset.12.repo_url': 'https://huggingface.co/m-a-p/Code-Feedback', 'llama.embedding_length': '3072', 'llama.attention.head_count': '24', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Dolphin, created by Eric Hartford. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Dolphin, created by Eric Hartford. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'general.quantization_version': '2', 'general.file_type': '17'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "model_path=os.path.join(\"..//models//Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf\")\n",
    "\n",
    "llm=llama_cpp.Llama(model_path=model_path, chat_format=\"llama-2\", n_ctx=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f64c59",
   "metadata": {},
   "source": [
    "Prompt to convert natural language to sql queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8a1be",
   "metadata": {},
   "source": [
    " ###Examples\n",
    "    Example question is enclosed inside ''''. Example output is enclosed inside '''. \n",
    "\n",
    "    #Example1:\n",
    "    ''''What are the names of the top 5 customers by revenue.''''\n",
    "    '''SELECT * FROM customers ORDER BY revenue DESC LIMIT 5;'''\n",
    "\n",
    "    #Example2:\n",
    "    ''''List the top 10 products sold in the last 6 months, including their names, categories, and total units sold, only for products that belong to the 'Electronics' category and have at least 100 units sold. Sort the results by total units sold in descending order.''''\n",
    "    '''SELECT * FROM products p JOIN categories c ON p.category_id = c.id JOIN sales s ON p.id = s.product_id WHERE c.category_name = 'Electronics' AND s.sale_date >= DATE_SUB(CURDATE(), INTERVAL 6 MONTH) GROUP BY p.name, c.category_name HAVING total_units_sold >= 100 ORDER BY total_units_sold DESC LIMIT 10;'''\n",
    "\n",
    "    #Example3:\n",
    "    ''''Select top 10 highest-paid employees.''''\n",
    "    '''SELECT * FROM employees ORDER BY salary DESC LIMIT 10;'''\n",
    "\n",
    "    #Example4:\n",
    "    ''''Get all engineers earning more than ₹1L.''''\n",
    "    '''SELECT name, department FROM employees WHERE department = 'Engineering' AND salary > 100000;'''\n",
    "\n",
    "    #Example5:\n",
    "    ''''Fetch total students and total books.''''\n",
    "    '''SELECT (SELECT COUNT(*) FROM students) AS total_students, (SELECT COUNT(*) FROM books) AS total_books;'''\n",
    "\n",
    "    #Example6:\n",
    "    ''''Create a unified log with a source column.''''\n",
    "    '''SELECT code, description, 'ERROR' AS type FROM errors UNION SELECT code, description, 'WARNING' AS type FROM warnings;'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e185b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbbc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always ensure your query always has at least {top_k} results if available. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "    Ensure the created query returns all available columns.\n",
    "    Ensure that the columns queried exist in the {table_info}.\n",
    "    Ensure that values queried exist in the {table_info}.\n",
    "    Apply only user specified conditions.\n",
    "    Ensure all criterion set by user has been utilised.\n",
    "    Make sure to enclose string literals in '''.\n",
    "    Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "    Only use the tables as listed in:\n",
    "    {table_info}.\n",
    "    The output should be enclosed inside '''.\n",
    "    The output should only contain the SQL query, enclosed within triple single quotes ('''), with no explanation, comments, unnecessary text or natural language.\n",
    "\n",
    "\n",
    "    ###Important\n",
    "    Only query tables and columns as listend in {table_info}.\n",
    "    \n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {input}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6ce78d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query(state: State)->State:\n",
    "    dialect=state[\"db\"].dialect\n",
    "    top_k=5\n",
    "    table_info=state[\"db_info\"]\n",
    "    input=state[\"question\"]\n",
    "\n",
    "    prompt_template=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "    Never query for all the columns from a specific table, only ask for a the few relevant columns given the question. Generate only one query.\n",
    "    Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "    Be careful to not query for values that are not in the scheme description. \n",
    "    Generate only one query.\n",
    "    Only use the following tables:\n",
    "    {table_info}\n",
    "\n",
    "    Schem description:\n",
    "    {table_info}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {input}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "    temp=0.5\n",
    "    max_tokens=300\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=prompt_template,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    state[\"query\"]=result\n",
    "    state[\"question\"]=input\n",
    "    llm.reset()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0772ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(state: State):\n",
    "    execute_query_tool=QuerySQLDataBaseTool(db=state[\"db\"])\n",
    "    #return {\"result\": execute_query_tool.invoke(state[\"query\"])} \n",
    "    result=execute_query_tool.invoke(state[\"query\"])\n",
    "    state[\"result\"]=result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "abbb502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state: State={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ce9bda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "da9b7a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  116983.60 ms\n",
      "llama_perf_context_print: prompt eval time =   52087.65 ms /  1309 tokens (   39.79 ms per token,    25.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6568.32 ms /    71 runs   (   92.51 ms per token,    10.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   58727.31 ms /  1380 tokens\n"
     ]
    }
   ],
   "source": [
    "state=assign_db(state)\n",
    "state[\"question\"]=\"Give me information on all indian vessels.\"\n",
    "state=write_query(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "26a2f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT latitude, longitude, location_name, location_country, location\n",
      "     FROM JANES_data\n",
      "     WHERE location_country = 'China'\n",
      "     ORDER BY latitude, longitude\n",
      "     LIMIT 5;\n"
     ]
    }
   ],
   "source": [
    "print(state[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "44435632",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=execute_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e6900acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(state[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a20dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: State)->State:\n",
    "    question=state[\"question\"]\n",
    "    context=state[\"result\"]\n",
    "    time=datetime.now().strftime(\"%d-%m-%y %H:%M:%S\")\n",
    "    prompt_template=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Act as an experienced Indian military tactician creating a report using {context}.\n",
    "    Explain it like someone who is a Indian naval commander.\n",
    "    Using the given {context}, answer the {question} in a precise manner using crisp military parlance.\n",
    "    Ensure that the answer contains information from the provided {context}.\n",
    "    The goal is to identify patterns in the {context} and relay necessary information.\n",
    "    It should be a concise report, consisting of all the necessary information, highlighting patterns in data.\n",
    "\n",
    "    You will be given a list of column labels, an explaination on what each label means, and then the {context}, which is a list of data tuples.\n",
    "    Each element in the data tuple, corresponds to the column label at the same position.\n",
    "\n",
    "    If the answer on the question is not in the provided context, tell the user, you can't answer the question on basis of the available data.\n",
    "    Structure your response in markdown.\n",
    "    Include Report Generation Time and date {time} in the report.\n",
    "\n",
    "    #column labels\n",
    "    id, name, latitude, longitude, range, bearing, course, speed, altitude, depth, reported_by, comment, hostility, category, nationality, location_wrt_naval_borders, closest_point_of_mil_interest, time, location\n",
    "\n",
    "    #explaination of each label\n",
    "    id: id is the unique identification number used for each target. Repeated occurences of the same id, mean information about the same target, and indicates a movement patter.\n",
    "    name: Non-unique identification for each target. Useful for explaining in a more meaningful manner.\n",
    "    latitude: latitudinal coordinate of each target. Non essential for the report.\n",
    "    longitude: longitudinal coordinate of each target. Non essential for the report.\n",
    "    range: Indicates how far the target is from the user.\n",
    "    bearing: Indicates the direction of the target's position with respect to the user, unit in degrees.\n",
    "    course: Indicates the direction the target is moving in, with respect to True North, unit in degrees.\n",
    "    speed: Speed of the target, unit in knots.\n",
    "    altitude: Height of the target, unit in feet.\n",
    "    depth: Depth of the target, unit in metres.\n",
    "    reported_by: Name of the entity which reported the target to the user.\n",
    "    comment: Additional insights about the target.\n",
    "    hostility: Indicates relationship between target and user. Important component to be considered in the report creation.\n",
    "    category: Identifies nature of the target. Can be used to identify movement pattern as explained below. Essential to the report.\n",
    "    nationality: Identifies origin country of the target.\n",
    "    location_wrt_naval_borders: Identifies if target is inside Indian naval boundaries. If it is, it is important to be flagged in the report.\n",
    "    closest_point_of_mil_interest: Identifies closest point of military interest to the target. Indicates prospective destination of the target. Might be used to indicate a pattern.\n",
    "    time: Identifies time of capture of information of the target. Useful to identify patterns.\n",
    "    location: Non essential for the report.\n",
    "\n",
    "    Based on category and movement patterns,\n",
    "    If the target is a ship, it's movement might be of the type\n",
    "    Transit: Moving from point A to B (e.g. \"underway transit through Strait of Hormuz\").\n",
    "    Patrol: Systematic movement within a designated area to monitor or deter.\n",
    "    Station Keeping: Maintaining a relative position to other ships or a fixed point.\n",
    "    Loitering: Holding in an area without aggressive movement; usually in anticipation.\n",
    "    Maneuvering: Tactical repositioning (can include evasive, offensive, or formation-related changes).\n",
    "    Shadowing: Following a foreign vessel at a set distance to monitor.\n",
    "\n",
    "    If the target is a submarine, it's movement might be of the type\n",
    "    Silent Running: No active sonar, minimal noise, passive listening only.\n",
    "    Depth Excursion: Changing depth suddenly to avoid detection or torpedoes.\n",
    "    Crazy Ivan: Sudden 180° turn at high speed to detect trailing enemies (Soviet/Russian tactic).\n",
    "    Bottoming: Settling quietly on the sea floor to hide.\n",
    "    Sprint-Drift: Burst of speed (sprint) followed by passive drift for listening.\n",
    "    Evasion Patterns: Zig-zagging or random depth/speed changes to break sonar lock.\n",
    "    Shadowing: Following a target vessel (esp. strategic like a carrier or SSBN).\n",
    "\n",
    "    if the target is an aircraft, it's movement might of the type\n",
    "    Dogfighting: Close-range air combat.\n",
    "    Split-S: Rolling inverted and diving to reverse course.\n",
    "    Immelmann Turn: Climbing half-loop followed by roll to reverse direction with altitude gain.\n",
    "    Barrel Roll: Spiral roll to evade.\n",
    "    Scissors: Close-range weaving maneuvers to force overshoot.\n",
    "    High-G Turn: Tight turning using gravity to bleed enemy speed or positioning.\n",
    "    Zoom Climb: Rapid vertical climb using momentum.\n",
    "\n",
    "    \n",
    "    ###Example:\n",
    "    Question: Give me all information on the movement of the Rafale.\n",
    "    Output:\n",
    "    **OPERATIONAL REPORT: RAFALE ACTIVITY ANALYSIS (12 JUN 2025)**\n",
    "\n",
    "    **TO:** Naval Command\n",
    "    **TIME AND DATE OF GENERATION OF REPORT:** 12th June 2025 17:07:43\n",
    "\n",
    "    **SUBJECT:** Assessment of Rafale Aircraft (ID 2001) Activity\n",
    "\n",
    "    **SUMMARY:**\n",
    "    Multiple reports by Arnab (ID 2001) indicate a friendly Indian Rafale aircraft conducting flights within Indian territorial waters near Porbandar. The aircraft exhibited erratic movements, varying speed, and fluctuating course over a brief observation period, suggesting potential training maneuvers or reconnaissance. All reported positions are well within Indian naval boundaries.\n",
    "\n",
    "    **OBSERVATIONS & ANALYSIS:**\n",
    "\n",
    "    1.  **Identity & Status:** Target ID 2001, identified as a Rafale aircraft of Indian nationality. Categorized as Friendly. This consistently indicates a known asset operating in our Area of Responsibility (AOR).\n",
    "    2.  **Geographic & Temporal Span:** Data points range from 12:02:47 to 12:06:47 IST, all within Indian Waters, proximate to **Porbandar**. This signifies continued presence in a critical coastal region.\n",
    "    3.  **Movement Patterns:**\n",
    "        * **Course Volatility:** The aircraft's course shows significant variance (from 149.7 to 57.4 degrees True North). This, coupled with the observation comments strongly suggests non-linear flight paths consistent with training, evasive maneuvers, or complex patrol patterns rather than a direct transit.\n",
    "        * **Speed Fluctuations:** Speed varied considerably, from a low of 385.0 knots to a high of 1300.8 knots. Such accelerations and decelerations are typical of air combat training or high-performance aerial reconnaissance.\n",
    "        * **Range & Altitude:** Range decreased and then increased, while altitude remained constant at 120001 feet (nominal for high-altitude air operations, implying it's not conducting ground-level observation).\n",
    "    4.  **Points of Interest:** The closest point of military interest remains consistently 'porbandar', reinforcing its operational focus in that vicinity.\n",
    "    5.  **Hostility Assessment:** Confirmed Friendly status throughout all reports, mitigating immediate threat assessment.\n",
    "\n",
    "    **CONCLUSION:**\n",
    "    The Rafale (ID 2001) is engaging in what appears to be standard operational flights, likely training or specialized reconnaissance, within authorized Indian airspace. The Erratic movements and variable speeds are characteristic of advanced aerial exercises. Continued monitoring is advised to confirm operational intent and detect any deviation from expected friendly patterns.\n",
    "\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {input}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "    temp=0.5\n",
    "    max_tokens=1500\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=prompt_template,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").strip()\n",
    "    \n",
    "    state[\"report\"]=result\n",
    "    state[\"question\"]=input\n",
    "    llm.reset()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fac8dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   84841.42 ms\n",
      "llama_perf_context_print: prompt eval time =  198032.74 ms /  4024 tokens (   49.21 ms per token,    20.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =  133179.75 ms /   693 runs   (  192.18 ms per token,     5.20 tokens per second)\n",
      "llama_perf_context_print:       total time =  333178.25 ms /  4717 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question:**\n",
      "\n",
      "     Given the data provided on the movement of a Rafale aircraft, we are tasked with analyzing the patterns and operational intent behind its movements within Indian territorial waters. Specifically, we are to identify any unusual or anomalous behavior that may indicate a deviation from standard operational procedures.\n",
      "\n",
      "     **Your Task:**\n",
      "\n",
      "     Utilize the information provided on the Rafale's movements to identify any patterns, operational intent, or anomalies that may be present. Consider the context of Indian airspace and territorial waters, as well as any relevant operational guidelines or protocols.\n",
      "\n",
      "     **Output:**\n",
      "\n",
      "     Please provide a concise report summarizing the observed patterns and any anomalies identified in the Rafale's movements within Indian airspace and territorial waters. Include any insights or considerations that may have contributed to the identification of such patterns or anomalies.\n",
      "\n",
      "     **Example:**\n",
      "\n",
      "     **Question:**\n",
      "\n",
      "     **Output:**\n",
      "\n",
      "     **OPERATIONAL REPORT: RAFALE ACTIVITY ANALYSIS (12 JUN 2025)**\n",
      "\n",
      "     **TO:** Naval Command\n",
      "     **TIME AND DATE OF GENERATION OF REPORT:** 12th June 2025 17:07:43\n",
      "\n",
      "     **SUBJECT:** Assessment of Rafale Aircraft (ID 2001) Activity\n",
      "\n",
      "     **SUMMARY:**\n",
      "\n",
      "     Multiple reports by Arnab (ID 2001) indicate a friendly Indian Rafale aircraft exhibiting erratic movements, varying speed, and fluctuating course within Indian territorial waters near Porbandar. Analysis suggests these movements may be indicative of training maneuvers or reconnaissance operations.\n",
      "\n",
      "     **OBSERVATIONS & ANALYSIS:**\n",
      "\n",
      "     1.  **Identity & Status:** Target ID 2001, identified as a Rafale aircraft of Indian nationality. Categorized as Friendly. This consistently indicates a known asset operating in our Area of Responsibility (AOR).\n",
      "     2.  **Geographic & Temporal Span:** Data points range from 12:02:47 to 12:06:47 IST, all within Indian Waters, proximate to **Porbandar**. This signifies continued presence in a critical coastal region.\n",
      "     3.  **Movement Patterns:**\n",
      "         * **Course Volatility:** The aircraft's course shows significant variance, suggesting non-linear flight paths consistent with training, evasive maneuvers, or complex patrol patterns rather than a direct transit.\n",
      "         * **Speed Fluctuations:** Speed varied considerably, from a low of 385.0 knots to a high of 1300.8 knots. Such accelerations and decelerations are typical of air combat training or high-performance aerial reconnaissance.\n",
      "         * **Range & Altitude:** Range decreased and then increased, while altitude remained constant at 120001 feet (nominal for high-altitude air operations, implying it's not conducting ground-level observation).\n",
      "     4.  **Points of Interest:** The closest point of military interest remains consistently 'porbandar', reinforcing its operational focus in that vicinity.\n",
      "     5.  **Hostility Assessment:** Confirmed Friendly status throughout all reports, mitigating immediate threat assessment.\n",
      "\n",
      "     **CONCLUSION:**\n",
      "\n",
      "     The Rafale (ID 2001) is engaging in what appears to be standard operational flights, likely training or specialized reconnaissance, within authorized Indian airspace. The Erratic movements and variable speeds are characteristic of advanced aerial exercises. Continued monitoring is advised to confirm operational intent and detect any deviation from expected friendly patterns.\n",
      "\n",
      "     **End of Report**\n"
     ]
    }
   ],
   "source": [
    "generate_answer(state)\n",
    "print(state[\"report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1669ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: State, input:str)->State:\n",
    "    '''\n",
    "    Decides what is to be done with user query, and assigns relevant route\n",
    "    '''\n",
    "\n",
    "    question=input\n",
    "    answer=state[\"report\"]\n",
    "    sql_prompt=state[\"query\"]\n",
    "\n",
    "    routing_prompt=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Act as a binary router for user questions, replying in 1 or 0.\n",
    "    The Response is to be binary in nature, and only reply in 1 or 0.\n",
    "    The task is to decide if the {question} is a request for explaination or for futher information or if it is finding new data, or modifying existing data.\n",
    "    Thus routing woud be of two kinds:\n",
    "    1. An explaination request\n",
    "        An explaination request, would imply the user asking for more information on a previous {answer}.\n",
    "\n",
    "    2. A new data request\n",
    "        A new data request, would imply the user is asking more data, not available in the previous {answer}. It would necessiate modifying the previous {sql_prompt}, and/or creating a new one.\n",
    "\n",
    "    If it is an explaination request, return 1\n",
    "    If it is a new data request, return 0\n",
    "\n",
    "    The foutput should be enclosed inside '''.\n",
    "    The format of the output is '''output'''.\n",
    "\n",
    "    Structure your output such that, the first line consists of either 1 or 0.\n",
    "    Any other information should only be from the second line onwards.\n",
    "\n",
    "    ###Examples\n",
    "    Example question is enclosed inside ''''. Example output is enclosed inside '''.\n",
    "    #Example1:\n",
    "    ''''Elaborate more on the selection of fruits available.''''\n",
    "    '''1'''\n",
    "\n",
    "    #Example2:\n",
    "    ''''No, This report does not contain sufficient information. I need to know more about the rafale. Regenerate the report accordingly.''''\n",
    "    '''0'''\n",
    "\n",
    "    #Example3:\n",
    "    ''''Tell me more about the movement of the rafale.''''\n",
    "    '''1'''\n",
    "\n",
    "    #Example4:\n",
    "    ''''Tell me about the submarine instead. Regenerate the report.''''\n",
    "    '''0'''\n",
    "\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {input}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    temp=0.3\n",
    "    max_tokens=100\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=routing_prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").strip()\n",
    "    match = re.search(r'[01]', result)\n",
    "    digit = int(match.group(0)) if match else None\n",
    "    \n",
    "    state[\"route\"]=digit\n",
    "    llm.reset()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44fa9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"question\"]=\"Give me more information on the chinese submarine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9fc3e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   74739.00 ms\n",
      "llama_perf_context_print: prompt eval time =   81025.31 ms /  1541 tokens (   52.58 ms per token,    19.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11957.59 ms /    94 runs   (  127.21 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   93112.79 ms /  1635 tokens\n"
     ]
    }
   ],
   "source": [
    "state=router(state, \"Give me more information on the chinese submarine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef364806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(state[\"route\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421eafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elaborate_on_response(state: State)->State:\n",
    "    '''\n",
    "    elaborates on given information, and assigns it to input\n",
    "    handles state[\"answer\"]\n",
    "    '''\n",
    "    question=state[\"question\"]\n",
    "    context=state[\"report\"]\n",
    "    data=state[\"result\"]\n",
    "    elaboration_prompt=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Act as an experienced Indian military tactician.\n",
    "    Explain it like someone who is a Indian naval commander.\n",
    "    Answer the user's {question} using the given {context} and {data}.\n",
    "    Be precise and concise in nature, with short, to the point responses.\n",
    "    Enclose the responses with '''.\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {question}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    temp=0.6\n",
    "    max_tokens=100\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=elaboration_prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").strip()\n",
    "    \n",
    "    state[\"answer\"]=result\n",
    "    llm.reset()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f48047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   74739.00 ms\n",
      "llama_perf_context_print: prompt eval time =   51363.06 ms /  1141 tokens (   45.02 ms per token,    22.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12493.39 ms /    95 runs   (  131.51 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   63998.62 ms /  1236 tokens\n"
     ]
    }
   ],
   "source": [
    "state=elaborate_on_response(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02cd23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chinese submarine, identified as ID 2002, was observed displaying aggressive movement patterns within Indian territorial waters. It exhibited significant depth fluctuations and course volatility, suggesting potential stealth reconnaissance or tactical maneuvers. Its speed varied considerably, indicative of active operational maneuvers. The submarine's presence was notably near Mumbai, reinforcing its operational focus in that vicinity. Due to the hostile nature of its activity, the report is flagged for immediate escalation to higher command levels for further action and intelligence gathering.\n"
     ]
    }
   ],
   "source": [
    "print(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b20e5904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OPERATIONAL REPORT: CHINESE SUBMARINE ACTIVITY ANALYSIS (12 JUN 2025)**\n",
      "\n",
      "     **TO:** Naval Command\n",
      "     **TIME AND DATE OF GENERATION OF REPORT:** 12th June 2025 18:56:11\n",
      "\n",
      "     **SUBJECT:** Assessment of Chinese Submarine Activity (ID 2002) - Silent Running and Depth Excursion\n",
      "\n",
      "     **SUMMARY:**\n",
      "     Reports by Arnab (ID 2002) indicate that Chinese submarines are conducting Silent Running and Depth Excursion maneuvers, indicative of advanced underwater warfare training or strategic reconnaissance. All reported positions are well within Chinese territorial waters, indicating continued operational focus in these areas.\n",
      "\n",
      "     **OBSERVATIONS & ANALYSIS:**\n",
      "\n",
      "     1.  **Identity & Status:** Target ID 2002, identified as Chinese submarines. Categorized as Unknown. This suggests an unknown asset operating in our Area of Responsibility (AOR), requiring careful observation and analysis.\n",
      "\n",
      "     2.  **Geographic & Temporal Span:** Data points range from 12:03:47 to 12:06:47 IST, all within Chinese Waters. This signifies continued presence in critical coastal regions, suggesting ongoing operational activities.\n",
      "\n",
      "     3.  **Movement Patterns:**\n",
      "        * **Silent Running:** The submarines are conducting Silent Running maneuvers, characterized by no active sonar, minimal noise, and passive listening only. This suggests advanced underwater warfare training or strategic reconnaissance, intended to remain undetected.\n",
      "        * **Depth Excursion:** The submarines are also conducting Depth Excursion maneuvers, characterized by sudden changes in depth to avoid detection or torpedoes. This suggests a tactical repositioning or evasion pattern, intended to break sonar locks or evade detection.\n",
      "\n",
      "     4.  **Points of Interest:** The closest point of military interest remains consistently within Chinese territorial waters, reinforcing its operational focus in these areas.\n",
      "\n",
      "     5.  **Hostility Assessment:** The unknown nature of the submarines (Unknown category) and the lack of hostile intent (Friendly status) throughout all reports mitigate immediate threat assessment.\n",
      "\n",
      "     **CONCLUSION:**\n",
      "     The Chinese submarines (ID 2002) are engaging in what appears to be advanced underwater warfare training or strategic reconnaissance, within authorized Chinese territorial waters. The Silent Running and Depth Excursion maneuvers are characteristic of complex underwater warfare training or strategic reconnaissance. Continued monitoring is advised to confirm operational intent and detect any deviation from expected unknown patterns.\n",
      "\n",
      "     **END OF REPORT.**\n"
     ]
    }
   ],
   "source": [
    "print(state[\"report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1aed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_report_to_pdf(state: State):\n",
    "    report=state[\"report\"]\n",
    "    report_conversion_prompt=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Act as a report formatter, responsible for converting text into a proper format.\n",
    "    The task is to format the user given {report} into the proper format as shown below.\n",
    "    The output should be in the correct format as shown in the below examples.\n",
    "    Make minimal changes to the actual information in the content, but encapsulate it in the below provided format.\n",
    "    Only convert the report as done in the below examples, and nothing else.\n",
    "\n",
    "    The proper format is:\n",
    "    From:  \tCommanding Officer, Naval Support Activity Monterey\n",
    "    To:     \tHere B. Recipient, Organization\n",
    "    Via:\t(1) Here B. Intermediary, Organization (if needed for intermediary endorsement)\n",
    "        (2) Number “via” recipients if more than 1; do not number if only 1\n",
    "\n",
    "    Subj:  \tLIMIT TO TWO LINES ALL CAPS NO ACRONYMS NO ABBREVIATIONS NO \n",
    "        PUNCTUATION (REPEAT SUBJECT LINE AT TOP OF SUBSEQUENT PAGES)\n",
    "\n",
    "    Ref:    \t(a) List as needed, or remove this line; must be referenced in the letter in order listed here\n",
    "                (b) Include references/excerpts in routing package if they will inform the CO’s decision\n",
    "\n",
    "    Encl:\t(1) List as needed, or remove this line; number all enclosures here, even if just 1 \n",
    "        (2) Must be referenced in the letter in order listed here\n",
    "\n",
    "    1.  Left and right margins are always set at 1 inch.  Times New Roman 12 pitch font is preferred for Navy correspondence.  Single spacing between lines.  Double spacing between paragraphs/subparagraphs.  Send editable electronic copy to Admin for formatting/editing.\n",
    "\n",
    "        a.\tIndented ¼ inch.  \n",
    "\n",
    "        b.\tIndented ¼ inch.  If there is an a, there should be a b.\n",
    "\n",
    "            (1) Indented ½ inch. \n",
    "\n",
    "            (2) Indented ½ inch.  If there is a (1), there should be a (2).\n",
    "\n",
    "                (a) Indented ¾ inch.\n",
    "\n",
    "                (b) Indented ¾ inch.  If there is an (a), there should be a (b).\n",
    "\n",
    "                    1.  Indented 1 inch.\n",
    "\n",
    "                    2.  Indented 1 inch.  If there is a 1, there should be a 2.\n",
    "\n",
    "    2.  This is the second page of this letter.\n",
    "\n",
    "    3.  For proper alignment, click on ruler across the top in Microsoft Word to set “soft” tab stops at ¼, ½, ¾, 1, 1 ¼ inches, etc.  Default tab stops set at 0.25” for each successive indentation.  Number pages 2 and up centered ½ inch from the bottom (including main letter and enclosures).\n",
    "\n",
    "    4.  Do not use automatic formatting, bulleting, or “hard” stops that change page margins.  If copying from another document, select “keep text only” option to maintain proper formatting.\n",
    "\n",
    "    5.  Break out acronyms on first use, then use the acronym the rest of the letter.\n",
    "                                                                                                    \n",
    "\n",
    "\n",
    "            I. M. COMMANDING\n",
    "\n",
    "    Copy to:  (List here, as needed; keep to the minimum number necessary)\n",
    "    Command Admin (N1/N04C)      Programs Integrator (N5)          CNRSW Chief of Staff              \n",
    "    Operations (N3)                            Information Technology (N6)   Tenant Commands\n",
    "    Public Works (N4)                       QOL Director (N9)                     NAVSUPPACT ANYWHERE\n",
    "\n",
    "    ###Examples\n",
    "    The correctly formatted output is enclosed in '''''.\n",
    "    #Example 1:\n",
    "    '''''\n",
    "\t12 Jun 25\n",
    "\n",
    "    From:\tCommanding Officer, Naval Support Activity Monterey  \n",
    "    To:    \tHere B. Recipient, Organization  \n",
    "    Via:\t(1) Here B. Intermediary, Organization  \n",
    "\n",
    "    Subj:  \tASSESSMENT OF AIRCRAFT ACTIVITY WITHIN INDIAN TERRITORIAL WATERS\n",
    "\n",
    "    Ref:\t(a) Aircraft Movement Logs dated 12 Jun 25  \n",
    "            (b) Surveillance Reports compiled by Indian Naval Aviation Command  \n",
    "\n",
    "    Encl:\t(1) Tabulated Movement Logs  \n",
    "            (2) Visual Reconnaissance Summary  \n",
    "\n",
    "    1.  This correspondence outlines the observed aircraft activity on 12 June 2025 within Indian territorial airspace, based on compiled data from naval surveillance systems and field reports. All contact was evaluated and determined to be of Friendly nature.  \n",
    "\n",
    "        a.\tAircraft types observed were exclusively of Indian origin, with no detection of foreign or unidentified aerial assets.\n",
    "\n",
    "        b.\tTime-stamped reports indicated consistent aircraft movement in strategic sectors across Indian coastal boundaries, confirming a pattern of presence across key naval areas of responsibility (AOR).\n",
    "\n",
    "            (1) Aerial maneuvers reported included advanced combat training routines:\n",
    "\n",
    "                (a) Dogfighting simulations consistent with air-to-air engagement exercises.  \n",
    "\n",
    "                (b) Split-S and Immelmann Turn maneuvers observed in multiple sectors.  \n",
    "\n",
    "                (c) Barrel Rolls, Scissors maneuvers, and High-G turns performed—indicative of evasive and tactical training operations.\n",
    "\n",
    "                (d) Zoom climbs observed, matching profiles typical of high-speed vertical ascent training.\n",
    "\n",
    "            (2) All maneuvers occurred within authorized Indian airspace. Proximity to high-value installations remained within standard operational norms.\n",
    "\n",
    "    2.  No hostile intent or unauthorized incursion was detected. All aircraft maintained expected communication and transponder compliance with the Indian Naval Aviation Command.  \n",
    "\n",
    "    3.  The pattern of erratic but deliberate movement, high-speed turns, and advanced aerobatics aligns with known combat-readiness training routines or specialized reconnaissance tasks.  \n",
    "\n",
    "    4.  Continued observation and logging are recommended to maintain situational awareness and flag any deviations from anticipated friendly patterns.\n",
    "\n",
    "    5.  No escalation or response action is warranted at this time.\n",
    "\n",
    "            I. M. COMMANDING\n",
    "\n",
    "    Copy to:  \n",
    "    Command Admin (N1/N04C)  \n",
    "    Programs Integrator (N5)  \n",
    "    CNRSW Chief of Staff  \n",
    "    Operations (N3)  \n",
    "    Information Technology (N6)  \n",
    "    Tenant Commands  \n",
    "    Public Works (N4)  \n",
    "    QOL Director (N9)  \n",
    "    NAVSUPPACT ANYWHERE\n",
    "\n",
    "    '''''\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Report: {report}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    temp=0.5\n",
    "    max_tokens=1024\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=report_conversion_prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5f4c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2368 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81066.31 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   84506.85 ms /   377 runs   (  224.16 ms per token,     4.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   85658.90 ms /   378 tokens\n"
     ]
    }
   ],
   "source": [
    "formatted=convert_report_to_pdf(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18f09187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From:\tNaval Command\n",
      "     To:\tHere B. Recipient, Organization\n",
      "     Via:\t(1) Here B. Intermediary, Organization\n",
      "\n",
      "     Subj:\tOPERATIONAL REPORT: CHINESE SUBMARINE ACTIVITY ANALYSIS (12 JUN 2025)\n",
      "\n",
      "     Ref:\t(a) Reports by Arnab (ID 2002)\n",
      "\n",
      "     Encl:\t(1) Operational Report: Chinese Submarine Activity Analysis (12 Jun 2025)\n",
      "\n",
      "     1.  This operational report provides an analysis of Chinese submarine activity on 12 June 2025, as reported by Arnab (ID 2002). The submarines were observed conducting Silent Running and Depth Excursion maneuvers, indicative of advanced underwater warfare training or strategic reconnaissance. All reported positions were within Chinese territorial waters, indicating continued operational focus in these areas.\n",
      "\n",
      "     2.  The activity of Chinese submarines (ID 2002) suggests an unknown asset operating in our Area of Responsibility (AOR), requiring careful observation and analysis.\n",
      "\n",
      "     3.  The submarines' presence in critical coastal regions suggests ongoing operational activities.\n",
      "\n",
      "     4.  The movement patterns of Chinese submarines (ID 2002) include Silent Running and Depth Excursion maneuvers. These suggest advanced underwater warfare training or strategic reconnaissance, intended to remain undetected.\n",
      "\n",
      "     5.  The closest point of military interest remains consistently within Chinese territorial waters.\n",
      "\n",
      "     6.  The unknown nature of the submarines and the lack of hostile intent throughout all reports mitigate immediate threat assessment.\n",
      "\n",
      "     **CONCLUSION:**\n",
      "     The Chinese submarines (ID 2002) are engaging in what appears to be advanced underwater warfare training or strategic reconnaissance, within authorized Chinese territorial waters. Continued monitoring is advised to confirm operational intent and detect any deviation from expected unknown patterns.\n",
      "\n",
      "     **END OF REPORT.**\n"
     ]
    }
   ],
   "source": [
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1437f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "871d191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def markdown_to_pdf(md_text: str, output_path: str):\n",
    "    # Convert markdown to HTML\n",
    "    html = markdown.markdown(md_text)\n",
    "\n",
    "    # Save HTML to temp file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\") as tmp:\n",
    "        tmp.write(html.encode(\"utf-8\"))\n",
    "        tmp_path = tmp.name\n",
    "\n",
    "    # Convert HTML to PDF\n",
    "    pdfkit.from_file(tmp_path, output_path)\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7195c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_to_pdf(\"**HELLO**\", \"report_html.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "777d44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_to_pdf(state[\"report\"], \"report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636529f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_result(state, file_path: str = \"generated_report.pdf\") -> bytes:\n",
    "    formatted_report=convert_report_to_pdf(state)\n",
    "\n",
    "    temp_html_file_path = None\n",
    "    temp_pdf_file_path = None\n",
    "\n",
    "    try:\n",
    "        html_content = markdown.markdown(formatted_report)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\", mode=\"w\", encoding=\"utf-8\") as tmp_html:\n",
    "            tmp_html.write(html_content)\n",
    "            temp_html_file_path = tmp_html.name\n",
    "        print(f\"Temporary HTML file created: {temp_html_file_path}\")\n",
    "\n",
    "        if file_path:\n",
    "            # If user wants to save to a specific file, use that path\n",
    "            output_pdf_actual_path = file_path\n",
    "            return_bytes = False\n",
    "        else:\n",
    "                # If user wants bytes, create another temporary file for PDF output\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\", mode=\"wb\") as tmp_pdf:\n",
    "                temp_pdf_file_path = tmp_pdf.name\n",
    "            output_pdf_actual_path = temp_pdf_file_path\n",
    "            return_bytes = True\n",
    "\n",
    "        print(f\"PDF output path for wkhtmltopdf: {output_pdf_actual_path}\")\n",
    "        \n",
    "        pdfkit.from_file(temp_html_file_path, output_pdf_actual_path)\n",
    "        print(f\"PDF generated by pdfkit successfully at: {os.path.abspath(output_pdf_actual_path)}\")\n",
    "\n",
    "        if return_bytes:\n",
    "            # Read the generated PDF file into bytes\n",
    "            with open(output_pdf_actual_path, \"rb\") as f:\n",
    "                pdf_bytes = f.read()\n",
    "            return pdf_bytes\n",
    "        else:\n",
    "            # PDF was saved to disk, return None as per function signature\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # This specific error usually means wkhtmltopdf is not found\n",
    "        error_msg = \"Error: 'wkhtmltopdf' executable not found. Please ensure it's installed and in your system PATH.\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from None # Re-raise for Streamlit to catch\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise # Re-raise the exception to be caught by Streamlit's try/except\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if temp_html_file_path and os.path.exists(temp_html_file_path):\n",
    "            os.remove(temp_html_file_path)\n",
    "            print(f\"Cleaned up temporary HTML file: {temp_html_file_path}\")\n",
    "        if temp_pdf_file_path and os.path.exists(temp_pdf_file_path):\n",
    "            os.remove(temp_pdf_file_path)\n",
    "            print(f\"Cleaned up temporary PDF file: {temp_pdf_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "56c3132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: State)->State:\n",
    "    '''\n",
    "    Decides what is to be done with user query, and assigns relevant route\n",
    "    handles state[\"route\"]\n",
    "    '''\n",
    "    print(\"Routing action to be taken\")\n",
    "    question=state[\"question\"]\n",
    "    information=state[\"result\"]\n",
    "\n",
    "    print(\"Initialised answer\")\n",
    "    routing_prompt=f\"\"\"\n",
    "    <|im_start|>system\n",
    "    You are a general-purpose AI that helps people with questions.\n",
    "\n",
    "    Given a question, your job is to categorize it into one of three categories:\n",
    "    1. report: For when the user instructs, that instruct for report generation.\n",
    "    2. data: For when the user requests for more data.\n",
    "    3. analysis: For when the user requests for more analysis.\n",
    "    4. distance: For when the user requests to find distance.\n",
    "    5. general: For all other questions.\n",
    "    Your response should be one word only.\n",
    "\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {question}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    temp=0.5\n",
    "    max_tokens=100\n",
    "    print(\"Creating response\")\n",
    "    response=llm.create_completion(\n",
    "        prompt=routing_prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    print(\"Created Response\")\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").strip()\n",
    "    print(result)\n",
    "    state[\"route\"]=result\n",
    "    print(\"finished routing\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ee8bef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"question\"]=\"I need more data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a3950691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more data.\n"
     ]
    }
   ],
   "source": [
    "print(state[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c80c0cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19.63267, 70.3389, '(19.63267, 70.3389)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:06:47'), (19.3789, 70.2477, '(19.3789, 70.2477)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:05:47'), (19.444, 70.2316, '(19.444, 70.2316)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:04:47'), (19.5589, 70.16, '(19.5589, 70.16)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:03:47'), (19.5856, 67.2376, '(19.5856, 67.2376)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'port qasim', '12:02:47')]\n"
     ]
    }
   ],
   "source": [
    "print(state[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "05ed98e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing action to be taken\n",
      "Initialised answer\n",
      "Creating response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  116983.60 ms\n",
      "llama_perf_context_print: prompt eval time =    4740.11 ms /   144 tokens (   32.92 ms per token,    30.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =      89.06 ms /     1 runs   (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4832.18 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Response\n",
      "data\n",
      "finished routing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'db': <langchain_community.utilities.sql_database.SQLDatabase at 0x2a219410850>,\n",
       " 'db_info': '\\nCREATE TABLE \"JANES_data\" (\\n\\tlatitude FLOAT, \\n\\tlongitude FLOAT, \\n\\tlocation_name TEXT, \\n\\tlocation_country TEXT, \\n\\tlocation TEXT\\n)\\n\\n/*\\n3 rows from JANES_data table:\\nlatitude\\tlongitude\\tlocation_name\\tlocation_country\\tlocation\\n18.9393\\t72.8445\\tmumbai\\tIndia\\t(18.9393, 72.8445)\\n19.0056\\t72.8163\\tjawaharlal nehru port (nhava sheva)\\tIndia\\t(19.0056, 72.8163)\\n17.6856\\t83.216\\tvisakhapatnam\\tIndia\\t(17.6856, 83.216)\\n*/\\n\\n\\nCREATE TABLE \"OTAS_data\" (\\n\\tid BIGINT, \\n\\tname TEXT, \\n\\tlatitude FLOAT, \\n\\tlongitude FLOAT, \\n\\trange FLOAT, \\n\\tbearing FLOAT, \\n\\tcourse FLOAT, \\n\\tspeed FLOAT, \\n\\taltitude BIGINT, \\n\\tdepth BIGINT, \\n\\treported_by TEXT, \\n\\tcomment TEXT, \\n\\thostility TEXT, \\n\\tcategory TEXT, \\n\\tnationality TEXT, \\n\\tlocation_wrt_naval_borders TEXT, \\n\\tclosest_point_of_mil_interest TEXT, \\n\\ttime TEXT, \\n\\tlocation TEXT\\n)\\n\\n/*\\n3 rows from OTAS_data table:\\nid\\tname\\tlatitude\\tlongitude\\trange\\tbearing\\tcourse\\tspeed\\taltitude\\tdepth\\treported_by\\tcomment\\thostility\\tcategory\\tnationality\\tlocation_wrt_naval_borders\\tclosest_point_of_mil_interest\\ttime\\tlocation\\n2004\\tGhazi\\t20.44\\t69.23\\t204.6\\t300.7\\t127.5\\t5.4\\t0\\t0\\tUtkarsh\\tSeems to be approaching territorial waters\\tHostile\\tSurface\\tPakistani\\tInside Indian Waters\\tjawaharlal nehru port (nhava sheva)\\t11:25:58\\t(20.44, 69.23)\\n2001\\tRafale\\t19.5856\\t67.2376\\t138.0\\t293.7\\t149.7\\t735.0\\t120001\\t0\\tArnab\\tErratic movements\\tFriendly\\tair\\tIndian\\tInside Indian Waters\\tport qasim\\t12:02:47\\t(19.5856, 67.2376)\\n2001\\tRafale\\t19.5589\\t70.16\\t127.9\\t290.3\\t166.9\\t385.0\\t120001\\t0\\tArnab\\tErratic movements\\tFriendly\\tair\\tIndian\\tInside Indian Waters\\tjawaharlal nehru port (nhava sheva)\\t12:03:47\\t(19.5589, 70.16)\\n*/',\n",
       " 'question': 'I need more data.',\n",
       " 'query': \"SELECT latitude, longitude, location, reported_by, comment, hostility, category, nationality, location_wrt_naval_borders, closest_point_of_mil_interest, time, location\\n     FROM OTAS_data\\n     WHERE name = 'Rafale'\\n     ORDER BY time DESC\\n     LIMIT 5;\",\n",
       " 'result': \"[(19.63267, 70.3389, '(19.63267, 70.3389)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:06:47'), (19.3789, 70.2477, '(19.3789, 70.2477)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:05:47'), (19.444, 70.2316, '(19.444, 70.2316)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:04:47'), (19.5589, 70.16, '(19.5589, 70.16)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'jawaharlal nehru port (nhava sheva)', '12:03:47'), (19.5856, 67.2376, '(19.5856, 67.2376)', 'Arnab', 'Erratic movements', 'Friendly', 'air', 'Indian', 'Inside Indian Waters', 'port qasim', '12:02:47')]\",\n",
       " 'route': 'data'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.reset()\n",
    "router(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "893f9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "def distance_between_locations(target_location, POI_location):\n",
    "    '''\n",
    "    Calculates distance between two given locations, likely between target be observed, and points of military interest\n",
    "    input: latitude, longitude of target, and latitude, longitude of POI\n",
    "    output: distance\n",
    "    '''\n",
    "    distance=geodesic(target_location, POI_location).kilometers\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "704f901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18329.492909296718\n"
     ]
    }
   ],
   "source": [
    "print(distance_between_locations(target_location=([82.3, 84.2]), POI_location=([-70.1, -54.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eb416940",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"question\"]=\"Find distance between new york and delhi. Delhi's latitude and longitude is 28.74 and 77.10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2341e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE \"JANES_data\" (\n",
      "\tlatitude FLOAT, \n",
      "\tlongitude FLOAT, \n",
      "\tlocation_name TEXT, \n",
      "\tlocation_country TEXT, \n",
      "\tlocation TEXT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from JANES_data table:\n",
      "latitude\tlongitude\tlocation_name\tlocation_country\tlocation\n",
      "18.9393\t72.8445\tmumbai\tindia\t(18.9393, 72.8445)\n",
      "19.0056\t72.8163\tjawaharlal nehru port (nhava sheva)\tindia\t(19.0056, 72.8163)\n",
      "17.6856\t83.216\tvisakhapatnam\tindia\t(17.6856, 83.216)\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE \"OTAS_data\" (\n",
      "\tid BIGINT, \n",
      "\tname TEXT, \n",
      "\tlatitude FLOAT, \n",
      "\tlongitude FLOAT, \n",
      "\trange FLOAT, \n",
      "\tbearing FLOAT, \n",
      "\tcourse FLOAT, \n",
      "\tspeed FLOAT, \n",
      "\taltitude BIGINT, \n",
      "\tdepth BIGINT, \n",
      "\treported_by TEXT, \n",
      "\tcomment TEXT, \n",
      "\thostility TEXT, \n",
      "\tcategory TEXT, \n",
      "\tnationality TEXT, \n",
      "\tlocation_wrt_naval_borders TEXT, \n",
      "\tclosest_point_of_mil_interest TEXT, \n",
      "\ttime TEXT, \n",
      "\ttarget_location TEXT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from OTAS_data table:\n",
      "id\tname\tlatitude\tlongitude\trange\tbearing\tcourse\tspeed\taltitude\tdepth\treported_by\tcomment\thostility\tcategory\tnationality\tlocation_wrt_naval_borders\tclosest_point_of_mil_interest\ttime\ttarget_location\n",
      "2004\tghazi\t20.44\t69.23\t204.6\t300.7\t127.5\t5.4\t0\t0\tutkarsh\tseems to be approaching territorial waters\thostile\tsurface\tpakistani\tinside indian waters\tjawaharlal nehru port (nhava sheva)\t11:25:58\t(20.44, 69.23)\n",
      "2001\trafale\t19.5856\t67.2376\t138.0\t293.7\t149.7\t735.0\t120001\t0\tarnab\terratic movements\tfriendly\tair\tindian\tinside indian waters\tport qasim\t12:02:47\t(19.5856, 67.2376)\n",
      "2001\trafale\t19.5589\t70.16\t127.9\t290.3\t166.9\t385.0\t120001\t0\tarnab\terratic movements\tfriendly\tair\tindian\tinside indian waters\tjawaharlal nehru port (nhava sheva)\t12:03:47\t(19.5589, 70.16)\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "print(state[\"db_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d887bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sql_query(state: State)->State:\n",
    "    llm.reset()\n",
    "    dialect=state[\"db\"].dialect\n",
    "    top_k=5\n",
    "    table_info=state[\"db_info\"]\n",
    "    input=state[\"question\"]\n",
    "\n",
    "    prompt_template=f\"\"\"\n",
    "    <|im_start|>system\n",
    "\n",
    "    ###TASK###\n",
    "    Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "    \n",
    "    ###INSTRUCTIONS###:\n",
    "    Never query for all the columns from a specific table, only ask for a the few relevant columns given the question. Generate only one query.\n",
    "    Pay attention to use only the column names, and their values that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "    Prefer structured filters using equality.\n",
    "    Only use the following tables:\n",
    "    {table_info}\n",
    "\n",
    "    ###SCHEMA DESCRIPTION###\n",
    "    {table_info}\n",
    "\n",
    "    ###USER FEEDBACK###\n",
    "    You should avoid fuzzy pattern matching with LIKE '%chinese%' unless strictly necessary.\n",
    "    You should not generate multiple queries.\n",
    "    \n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Question: {input}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "    temp=0.3\n",
    "    max_tokens=300\n",
    "\n",
    "    response=llm.create_completion(\n",
    "        prompt=prompt_template,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    result=response['choices'][0]['text']\n",
    "    result=result.replace(\"[/INST]\", \"\").replace(\"'''\", \"\").replace(\"```\", \"\").strip()\n",
    "    result = re.sub(r'\\b(submarines?|ships?|aircraft|helicopters?)\\b',\n",
    "           lambda m: {'submarine': 'subsurface', 'submarines': 'subsurface',\n",
    "                      'ship': 'surface', 'ships': 'surface',\n",
    "                      'aircraft': 'air', 'helicopter': 'air', 'helicopters': 'air'}[m.group(0).lower()],\n",
    "           result, flags=re.IGNORECASE)\n",
    "    result=result.lower()\n",
    "    state[\"query\"]=result\n",
    "    state[\"question\"]=input\n",
    "    llm.reset()\n",
    "    print(\"---------------------------------\\n\\n\")\n",
    "    print(state[\"query\"])\n",
    "    print(\"---------------------------------\\n\\n\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "940c873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state: State={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6186d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   58072.58 ms\n",
      "llama_perf_context_print: prompt eval time =   56713.74 ms /  1351 tokens (   41.98 ms per token,    23.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2054.66 ms /    15 runs   (  136.98 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   58791.85 ms /  1366 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "\n",
      "\n",
      "select * from otas_data where reported_by = 'utkarsh';\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "---------------------------question-------------------------\n",
      "\n",
      "\n",
      "Generate a report on what utkarsh has reported.\n",
      "---------------------------generated query-------------------------\n",
      "\n",
      "\n",
      "select * from otas_data where reported_by = 'utkarsh';\n",
      "---------------------------generated response-------------------------\n",
      "\n",
      "\n",
      "[(2004, 'ghazi', 20.44, 69.23, 204.6, 300.7, 127.5, 5.4, 0, 0, 'utkarsh', 'seems to be approaching territorial waters', 'hostile', 'surface', 'pakistani', 'inside indian waters', 'jawaharlal nehru port (nhava sheva)', '11:25:58', '(20.44, 69.23)')]\n"
     ]
    }
   ],
   "source": [
    "state[\"question\"]=\"Generate a report on what utkarsh has reported.\"\n",
    "state=assign_db(state)\n",
    "state=write_sql_query(state)\n",
    "state=execute_query(state)\n",
    "print(\"---------------------------question-------------------------\\n\\n\")\n",
    "print(state[\"question\"])\n",
    "print(\"---------------------------generated query-------------------------\\n\\n\")\n",
    "print(state[\"query\"])\n",
    "print(\"---------------------------generated response-------------------------\\n\\n\")\n",
    "print(state[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3262c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client=Client(api_key=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90230dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OPERATIONAL REPORT: UTKARSH ACTIVITY ANALYSIS (19 JUN 2025)**\n",
      "\n",
      "     **TO:** Naval Command\n",
      "     **TIME AND DATE OF GENERATION OF REPORT:** 19th June 2025 15:24:27\n",
      "\n",
      "     **SUBJECT:** Assessment of Utkarsh Class Corvette (ID 3002) Activity\n",
      "\n",
      "     **SUMMARY:**\n",
      "     Utkarsh (ID 3002) has been observed conducting a series of maneuvers within Indian territorial waters. The vessel exhibited a pattern of rapid accelerations and decelerations, coupled with significant course changes, indicative of advanced tactical training or complex operational drills.\n",
      "\n",
      "     **OBSERVATIONS & ANALYSIS:**\n",
      "\n",
      "     1.  **Identity & Status:** Target ID 3002, identified as a Utkarsh class Corvette of Indian nationality. Categorized as Active. The consistently friendly status indicates an operational asset.\n",
      "\n",
      "     2.  **Geographic & Temporal Span:** Data points range from 19:00:00 to 19:10:00 IST, all within Indian Waters, proximate to **Gujarat Coast**. This signifies continued presence in a critical coastal region.\n",
      "\n",
      "     3.  **Movement Patterns:**\n",
      "         * **Course Volatility:** The Corvette's course shows significant variance (from 315.3 to 45.4 degrees True North). This, coupled with the observation comments, strongly suggests non-linear flight paths consistent with advanced tactical training, complex patrol patterns, or operational drills.\n",
      "         * **Speed Fluctuations:** Speed varied considerably, from a low of 45 knots to a high of 120 knots. Such accelerations and decelerations are typical of advanced naval maneuvers.\n",
      "         * **Range & Altitude:** Range varied, while altitude remained constant at 30 feet (nominal for surface-level operations, implying it's not conducting ground-level observation).\n",
      "\n",
      "     4.  **Points of Interest:** The closest point of military interest remains consistently 'gujarat coast', reinforcing its operational focus in that vicinity.\n",
      "\n",
      "     5.  **Hostility Assessment:** Confirmed Active status throughout all reports, mitigating immediate threat assessment. However, the erratic movements and variable speeds are indicative of a trained asset, capable of handling a variety of naval scenarios.\n",
      "\n",
      "     **CONCLUSION:**\n",
      "     The Utkarsh (ID 3002) is engaging in what appears to be standard operational maneuvers, likely training or specialized operational drills, within authorized Indian waters. The erratic movements and variable speeds are characteristic of advanced naval training. Continued monitoring is advised to confirm operational intent and detect any deviation from expected friendly patterns.\n",
      "\n",
      "     **NOTE:** Further analysis or reporting may be warranted based on the specific maneuvers observed or if the vessel deviates from its friendly status.\n"
     ]
    }
   ],
   "source": [
    "print(\"**OPERATIONAL REPORT: UTKARSH ACTIVITY ANALYSIS (19 JUN 2025)**\\n\\n     **TO:** Naval Command\\n     **TIME AND DATE OF GENERATION OF REPORT:** 19th June 2025 15:24:27\\n\\n     **SUBJECT:** Assessment of Utkarsh Class Corvette (ID 3002) Activity\\n\\n     **SUMMARY:**\\n     Utkarsh (ID 3002) has been observed conducting a series of maneuvers within Indian territorial waters. The vessel exhibited a pattern of rapid accelerations and decelerations, coupled with significant course changes, indicative of advanced tactical training or complex operational drills.\\n\\n     **OBSERVATIONS & ANALYSIS:**\\n\\n     1.  **Identity & Status:** Target ID 3002, identified as a Utkarsh class Corvette of Indian nationality. Categorized as Active. The consistently friendly status indicates an operational asset.\\n\\n     2.  **Geographic & Temporal Span:** Data points range from 19:00:00 to 19:10:00 IST, all within Indian Waters, proximate to **Gujarat Coast**. This signifies continued presence in a critical coastal region.\\n\\n     3.  **Movement Patterns:**\\n         * **Course Volatility:** The Corvette's course shows significant variance (from 315.3 to 45.4 degrees True North). This, coupled with the observation comments, strongly suggests non-linear flight paths consistent with advanced tactical training, complex patrol patterns, or operational drills.\\n         * **Speed Fluctuations:** Speed varied considerably, from a low of 45 knots to a high of 120 knots. Such accelerations and decelerations are typical of advanced naval maneuvers.\\n         * **Range & Altitude:** Range varied, while altitude remained constant at 30 feet (nominal for surface-level operations, implying it's not conducting ground-level observation).\\n\\n     4.  **Points of Interest:** The closest point of military interest remains consistently 'gujarat coast', reinforcing its operational focus in that vicinity.\\n\\n     5.  **Hostility Assessment:** Confirmed Active status throughout all reports, mitigating immediate threat assessment. However, the erratic movements and variable speeds are indicative of a trained asset, capable of handling a variety of naval scenarios.\\n\\n     **CONCLUSION:**\\n     The Utkarsh (ID 3002) is engaging in what appears to be standard operational maneuvers, likely training or specialized operational drills, within authorized Indian waters. The erratic movements and variable speeds are characteristic of advanced naval training. Continued monitoring is advised to confirm operational intent and detect any deviation from expected friendly patterns.\\n\\n     **NOTE:** Further analysis or reporting may be warranted based on the specific maneuvers observed or if the vessel deviates from its friendly status.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
