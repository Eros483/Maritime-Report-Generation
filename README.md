# ğŸ¤– Military Report Generation ChatBot
Allows users to communicate with databases in natural language, to get detailed reports and analysis as per their requests.

Created for WESEE, MoD.

---
## ğŸ“¦ Respository
Clone the deployment ready project
```
git clone https://github.com/Eros483/Maritime-Report-Generation
cd Maritime-Report-Generation
```
---
## âš™ï¸ Environment Setup
1. We recommend using Anaconda for environment control.
2. Create the environment using the provided yml, and activate it.
    - Take care to remove llama-cpp-python from the yml
```
conda env create -f environment.yml
conda activate rag_new_env
```
3. To install llama-cpp-python:
```
call "C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\Build\vcvars64.bat"
where cl
where cmake
set CMAKE_ARGS=-DGGML_CUDA=ON
set FORCE_CMAKE=1
pip install llama-cpp-python --no-cache-dir --verbose
```
---
## ğŸš€ Running the Frontend
```
streamlit run frontend.py
```
## ğŸ› ï¸ Frontend Preview
![Preview of Features](preview.png)
---
## ğŸ’¡ Project Structure
```
Maritime-Report-Generation
â”‚   .gitignore
â”‚   environment.yml
â”‚   frontend.py
â”‚   preview.png
â”‚   README.md
â”‚   requirements.txt
â”‚
â”œâ”€â”€â”€backend
â”‚       functions.py
â”‚       main.py
â”‚
â”œâ”€â”€â”€datasets
â”‚       data.csv
â”‚       data_processed.csv
â”‚       REDACTED.csv
â”‚       india_eez.json
â”‚
â”œâ”€â”€â”€MCP
â”‚   â”‚   client.py
â”‚   â”‚   config.json
â”‚   â”‚   frontend.py
â”‚   â”‚   mcp_state.json
â”‚   â”‚   state_manager.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€analysis_server
â”‚   â”‚       server.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€elaboration_server
â”‚   â”‚       server.py
â”‚   â”‚
â”‚   â””â”€â”€â”€report_generation_server
â”‚           server.py
â”‚
â”œâ”€â”€â”€models
â”‚       dolphin3.0-llama3.2-3b-q5_k_m.gguf
â”‚
â”œâ”€â”€â”€notebooks
â”‚       csv_to_sql_conversion.ipynb
â”‚       data_processing.ipynb
â”‚       generated_report.pdf
â”‚       myDataBase.db
â”‚       sql_communication.ipynb
â”‚
â””â”€â”€â”€sql_files
        myDataBase.db
```
---
## ğŸ§‘ğŸ»â€ğŸ’» Working Explaination
#### Objective: Existing Information Analysis and Reports are generated through manually parsing data for required fields, and then analysed by field experts, wasting precious time that can be automated.

#### Solution:
1. Using local llms via llama-cpp-python and Ollama to act as the brain for this agentic AI endeavour.
2. Parsed CSV data, preprocessing it and adding extra attributes as deemed necessary. Done in `notebooks/data_processing.ipynb`.
3. Utilised `sqlalchemy` to push csv data to `.db` sqlite files. Done in `notebooks/csv_to_sql_processing.ipynb`.
4. Prototyped  the following features in `notebooks/sql_communication.iynb`, and encapsulated them in `backend/functions.py`.
    - `load_model`: loads LLM Llama 3.2 using llama-cpp-python.
    - `write_sql_query`: Converts natural language query into sql queries.
        - Utilised the AUTOMAT framework for prompt engineering, along with existing prompts at LangChain.
        - Faced issues with llm output formatting,and table name hallucinations. 
    - `execute_query`: Executes created SQL query with Querying tool from langchain_community.
    - `report_generation`: Uses information released by `execute_query` to find significant patterns, identifies anomalies, and generates a report in a pre-dictated format, in markdown.
    - `elaborate_on_response` : Uses information released by `execute_query` and the report generated by `report_generation` to focus on a singular aspect as requested by user.
5. The next step was to combine all these methods into a single pipeline, for which two methods were devised:
    - LLM powered routing, based on user queries.
    - MCP architecture, employing agentic AI via OLlama LLM.
---
## ğŸ”€LLM powered Routing
1. Created nodes containing all the feature functions with edges, using LangGraph, and a globally used `state` via a typedDict configuration, and connected them using langgraph edges.
2. Added a conditional edge based on a function `router`:
    - `router`: Decides whether the user query was a report request, an analysis request or a general request, and routes nodes accordingly.
3. Pipeline contained in `backend` directory, with `frontend.py` being used as a frontend for accessing it.

**Drawback**: As `router` is LLM driven, might not work accurately for higher complexity queries, uneccesarily employing extra resources.

---
## ğŸ›ï¸ MCP Architecture
1. Created a client-server architecture based on MCP concepts.
2. Created multple servers housing the feature functions, with `state_manager.py` handling global data storage in `mcp_state.json`.
3. Created `client.py` which connects to all the servers, based on the contents of `config.json`. 
4. Pipeline accessible in `MCP` directory, with `frontend.py` providing usability.

**Drawback**: As MCP employes Agentic AI, the formatted reports received from functions, ceased to be the final output, and instead a LLM interpretation from the generated report became the receive output.

---
## ğŸ† Final Usage:
Due to lower inference time and superior performance, the manual routing logic pipeline was chosen over the MCP architecture for final usage.

---